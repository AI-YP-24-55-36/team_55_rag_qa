import time
import pickle
import numpy as np
from fastembed import SparseTextEmbedding, LateInteractionTextEmbedding, TextEmbedding
# from sentence_transformers import CrossEncoder
from fastembed.rerank.cross_encoder import TextCrossEncoder
from qdrant_client import models
from qdrant_client.models import (
    Distance,
    Modifier,
    OptimizersConfigDiff,
    HnswConfigDiff,
    MultiVectorConfig,
    SparseIndexParams,
    SparseVectorParams,
    VectorParams,
    PointStruct
)

from tqdm import tqdm
from logger_init import setup_paths, setup_logging


BASE_DIR, LOGS_DIR, GRAPHS_DIR, OUTPUT_DIR, EMBEDDINGS_DIR = setup_paths()
logger = setup_logging(LOGS_DIR, OUTPUT_DIR)

# –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
# reranker_model = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L-2-v2") - —Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è
reranker_model = TextCrossEncoder(model_name='jinaai/jina-reranker-v1-turbo-en')
# BAAI/bge-reranker-base - –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–∞—è
# jinaai/jina-reranker-v1-turbo-en - —Å—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å

# —Ñ—É–Ω–∫—Ü–∏—è —É–¥–∞–ª—è–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
def clear_existing_collections(client,  collection_name):
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]

    if collection_name in collection_names:
        print(f"–ö–æ–ª–µ–∫—Ü–∏—è {collection_name} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —É–¥–∞–ª—è–µ–º" )
        logger.info(f"–ö–æ–ª–µ–∫—Ü–∏—è {collection_name} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —É–¥–∞–ª—è–µ–º" )

        client.delete_collection(collection_name)
        logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É–¥–∞–ª–µ–Ω–∞")

# —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
def create_hybrid_collection(client, collection_name):
    client.create_collection(
        collection_name=collection_name,
        vectors_config={
            "dense": VectorParams(
                size=1024,
                distance=Distance.COSINE
            ),

            "colbertv2.0": VectorParams(
                size=128,
                distance=Distance.COSINE,
                multivector_config=MultiVectorConfig(comparator="max_sim"),
                hnsw_config=HnswConfigDiff(
                    m=0  # –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ HNSW
                )
            ),
        },
        sparse_vectors_config={
            "bm25": SparseVectorParams(
                index=SparseIndexParams(on_disk=False,

            ),
                modifier=Modifier.IDF,
                )

        },
        optimizers_config=OptimizersConfigDiff(
            indexing_threshold=0
        )

    )
    logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}, –≥–æ—Ç–æ–≤–∞ –∫ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—é")
    print(f"–°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}, –≥–æ—Ç–æ–≤–∞ –∫ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—é")

dense_embeddings = np.memmap('embeddings/mxbai-embed-large-v1.memmap', dtype='float32', mode='r').reshape(-1, 1024)
colbert_embeddings = np.memmap('embeddings/colbert_embeddings.memmap', dtype='float32', mode='r').reshape(-1, 256, 128)
with open('embeddings/sparse_embeddings.pkl', 'rb') as f:
    sparse_embeddings = pickle.load(f)

def build_point_from_files(
    item,
    idx,
    sparse_embeddings,
    dense_embeddings,
    colbert_embeddings
):
    # –∑–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å—É –∏–∑ —Ñ–∞–π–ª–æ–≤ memmap
    sparse_embedding = sparse_embeddings[idx]
    dense_embedding = dense_embeddings[idx].tolist()
    colbert_embedding = colbert_embeddings[idx].tolist()

    return PointStruct(
        id=item["id"],
        payload=item,
        vector={
            "bm25": {
                "values": sparse_embedding.values.tolist() if sparse_embedding else [],
                "indices": sparse_embedding.indices.tolist() if sparse_embedding else []
            },
            "dense": dense_embedding,
            "colbertv2.0": colbert_embedding
        }
    )

# –∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏–Ω—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏
def upload_points_in_batches(client, collection_name, points, batch_size=50):
    for i in range(0, len(points), batch_size):
        batch = points[i:i + batch_size]
        client.upload_points(
            collection_name=collection_name,
            points=batch,
        )
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {i + len(batch)} –∏–∑ {len(points)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")


# —Å–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
def upload_hybrid_data(client, collection_name: str, data):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Qdrant —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (BM25 + Dense + ColBERT)"""
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name} —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–∏—Å–∫–æ–º")
    clear_existing_collections(client, collection_name)
    create_hybrid_collection(client, collection_name)
    logger.info(f"‚è≥ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ {collection_name}")
    print(f"‚è≥ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –∑–∞–≥—Ä—É–∑–∫–∏  {collection_name}")
    points = []
    for idx, item in tqdm(enumerate(data)):
        point = build_point_from_files(item, idx, sparse_embeddings, dense_embeddings, colbert_embeddings)
        points.append(point)
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(points)} points")
    upload_points_in_batches(client, collection_name, points)
    logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name}")
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name}")

    # –∑–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    client.update_collection(
        collection_name=collection_name,
        optimizer_config=OptimizersConfigDiff(indexing_threshold=5000),
    )




#  —Ñ—É–Ω—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
def load_embedding():
    return {
        "bm25": SparseTextEmbedding("Qdrant/bm25"),
        "dense": TextEmbedding(model_name="mixedbread-ai/mxbai-embed-large-v1"),
        "colbert": LateInteractionTextEmbedding("colbert-ir/colbertv2.0")
    }
# –∫–æ–¥–∏—Ä–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
def encode_query(query_text, models):
    sparse_vector = list(models["bm25"].query_embed(query_text))
    sparse_embedding = sparse_vector[0] if sparse_vector else None
    # dense_embedding = models["dense"].encode_corpus([{"text": query_text}], convert_to_tensor=False)[0]
    dense_embedding = list(models["dense"].embed(query_text, normalize=True))[0]
    colbert_embedding = list(models["colbert"].embed(query_text))[0]
    return sparse_embedding, dense_embedding, colbert_embedding

# –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –ë–î
def run_hybrid_search(client, collection_name, sparse_embedding, dense_embedding, colbert_embedding, top_k):
    prefetch = [
        models.Prefetch(query=dense_embedding, using="dense", limit=top_k),
        models.Prefetch(
            query=models.SparseVector(
                indices=sparse_embedding.indices.tolist() if sparse_embedding else [],
                values=sparse_embedding.values.tolist() if sparse_embedding else []
            ),
            using="bm25",
            limit=top_k,
        ),
    ]

    start_time = time.time()
    search_results = client.query_points(
        collection_name,
        prefetch=prefetch,
        query=colbert_embedding,
        using="colbertv2.0",
        with_payload=True,
        limit=top_k
    )
    end_time = time.time()

    query_time = end_time - start_time
    found_contexts = [(point.payload.get('context', ''), point.score) for point in search_results.points]

    return found_contexts, query_time

def evaluate_accuracy(found_contexts, true_context, top_k_values, results, stage):
    if not found_contexts:
        return
    if not all(isinstance(item, (tuple, list)) and len(item) == 2 for item in found_contexts):
        raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç found_contexts: –æ–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (context, score), –ø–æ–ª—É—á–µ–Ω–æ: {found_contexts}")

    for k in top_k_values:
        results["accuracy"][stage][k]["total"] += 1
        top_k_contexts = [ctx for ctx, _ in found_contexts[:k]]
        if true_context in top_k_contexts:
            results["accuracy"][stage][k]["correct"] += 1


def update_speed_metrics(results):
    query_times = results["speed"]["query_times"]
    if query_times:
        results["speed"]["avg_time"] = sum(query_times) / len(query_times)
        results["speed"]["median_time"] = sorted(query_times)[len(query_times) // 2]
        results["speed"]["max_time"] = max(query_times)
        results["speed"]["min_time"] = min(query_times)
    del results["speed"]["query_times"]


def log_final_metrics(results, top_k_values):
    for stage in ["before_rerank", "after_rerank"]:
        for k in top_k_values:
            correct = results["accuracy"][stage][k]["correct"]
            total = results["accuracy"][stage][k]["total"]
            accuracy = correct / total if total > 0 else 0
            results["accuracy"][stage][k]["accuracy"] = accuracy
            logger.info(f"Hybrid Search {stage.replace('_', ' ')} (top-{k}): {accuracy:.4f} ({correct}/{total})")

    logger.info(f"Hybrid Search –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['speed']['avg_time'] * 1000:.2f} –º—Å")
    logger.info(f"Hybrid Search –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['speed']['median_time'] * 1000:.2f} –º—Å")
    logger.info(f"Hybrid Search –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['speed']['max_time'] * 1000:.2f} –º—Å")
    logger.info(f"Hybrid Search –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['speed']['min_time'] * 1000:.2f} –º—Å")

# –∑–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
def benchmark_hybrid_rerank(client, collection_name, test_data, top_k_values, reranker=None):
    print(f"\nüîç –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ì–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ü–æ–∏—Å–∫–∞ + –†–µ—Ä–∞–Ω–∫–∞ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")
    logger.info(f"–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ì–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ü–æ–∏—Å–∫–∞ + –†–µ—Ä–∞–Ω–∫–∞ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")

    results = {
        "speed": {
            "avg_time": 0, "median_time": 0, "max_time": 0, "min_time": 0, "query_times": []
        },
        "accuracy": {
            "before_rerank": {k: {"correct": 0, "total": 0, "accuracy": 0} for k in top_k_values},
            "after_rerank": {k: {"correct": 0, "total": 0, "accuracy": 0} for k in top_k_values}
        }
    }

    max_top_k = max(top_k_values)
    total_queries = len(test_data)

    logger.info(f"–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Hybrid Search –¥–ª—è {total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤")
    print(f"‚è±Ô∏è  –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞...")

    embedding_models = load_embedding()
    progress_bar = tqdm(total=total_queries, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤", unit="–∑–∞–ø—Ä–æ—Å")

    for _, row in test_data.iterrows():
        query_text = row['question']
        true_context = row['context']

        sparse_embedding, dense_embedding, colbert_embedding = encode_query(query_text, embedding_models)
        found_contexts, query_time = run_hybrid_search(client, collection_name, sparse_embedding, dense_embedding, colbert_embedding, max_top_k)
        results["speed"]["query_times"].append(query_time)

        evaluate_accuracy(found_contexts, true_context, top_k_values, results, stage="before_rerank")

        if reranker:
            reranked_contexts = reranker(query_text, found_contexts)
            evaluate_accuracy(reranked_contexts, true_context, top_k_values, results, stage="after_rerank")

        progress_bar.update(1)

    progress_bar.close()

    update_speed_metrics(results)
    log_final_metrics(results, top_k_values)

    print(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Hybrid Search –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")
    return results

# —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–¥ TextCrossEncoder
def reranker(query, candidates, top_k=None):
    #  –ø–∞—Ä—ã (query, context)
    texts = [context for context, _ in candidates]

    #  –æ—Ü–µ–Ω–∫–∏ –æ—Ç –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ .rerank()
    new_scores = list(reranker_model.rerank(query, texts))

    # —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å—ã –∏ –æ—Ü–µ–Ω–∫–∏
    ranking = [(i, score) for i, score in enumerate(new_scores)]
    ranking.sort(key=lambda x: x[1], reverse=True)

    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –æ—Ü–µ–Ω–∫–∞–º
    reranked = [(texts[i], score) for i, score in ranking]

    if top_k is not None:
        return reranked[:top_k]
    return reranked

#
# # —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–¥ CrossEncoder
# def reranker(query, candidates, top_k=None):
#     texts = [(query, context) for context, _ in candidates]
#     scores = reranker_model.predict(texts)
#     # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–∏–º
#     reranked_results = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
#     # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ—Ä—Ç–µ–∂–∏: (context, new_score)
#     return [(context, score) for (context, _), score in reranked_results]


def print_comparison(results_without_rerank, results_with_rerank, top_k_values):
    print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Hybrid Search —Å —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º –∏ –±–µ–∑ –Ω–µ–≥–æ:\n")

    print("‚è≥ –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞:")
    print(f"  - –ë–µ–∑ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {results_without_rerank['speed']['avg_time'] * 1000:.2f} –º—Å")
    print(f"  - –° —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º: {results_with_rerank['speed']['avg_time'] * 1000:.2f} –º—Å")

    print("\nüéØ –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ (Accuracy):")
    for k in top_k_values:
        acc_before = results_without_rerank["accuracy"]["before_rerank"][k]["accuracy"]
        acc_after = results_with_rerank["accuracy"]["after_rerank"][k]["accuracy"]

        print(f"  - Top-{k}:")
        print(f"    - –ë–µ–∑ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {acc_before:.4f}")
        print(f"    - –° —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º: {acc_after:.4f}")


def run_bench_hybrid(client, data_for_db, data_df, load, top_k_values):
    if load == 1:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        upload_hybrid_data(
            client=client,
            collection_name="hybrid_collection",
            data=data_for_db
        )
    else:
        logger.info(f"üîç –ù–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ, –ø–∞—Ä–∞–º–µ—Ç—Ä load=0")
        print(f"\nüîç–ù–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ, –ø–∞—Ä–∞–º–µ—Ç—Ä load=0")

    results_without_rerank = benchmark_hybrid_rerank(
        client=client,
        collection_name="hybrid_collection",
        test_data=data_df,
        top_k_values=top_k_values,
        reranker=None
    )

    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫ —Å —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º
    results_with_rerank = benchmark_hybrid_rerank(
        client=client,
        collection_name="hybrid_collection",
        test_data=data_df,
        top_k_values=top_k_values,
        reranker=reranker  # –ø–µ—Ä–µ–¥–∞—á–∞ —Ñ—É–Ω–∫—Ü–∏—é —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
    )

    return results_without_rerank, results_with_rerank