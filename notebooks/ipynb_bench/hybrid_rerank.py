import datetime
import logging
import sys
import time
from pathlib import Path
import pickle

import matplotlib.pyplot as plt
import numpy as np
from fastembed import SparseTextEmbedding, LateInteractionTextEmbedding
from beir.retrieval.models import SentenceBERT
from sentence_transformers import CrossEncoder
from qdrant_client import models
from qdrant_client.models import (
    Distance,
    Modifier,
    MultiVectorConfig,
    SparseIndexParams,
    SparseVectorParams,
    VectorParams
)
from tqdm import tqdm

from log_output import Tee
from load_config import load_config

config = load_config()
BASE_DIR = Path(config["paths"]["base_dir"])
LOGS_DIR = BASE_DIR / config["paths"]["logs_dir"]
GRAPHS_DIR = BASE_DIR / config["paths"]["graphs_dir"]
OUTPUT_DIR = BASE_DIR / config["paths"]["output_dir"]
EMBEDDINGS_DIR = BASE_DIR / config["paths"]["embeddings_dir"]
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
sys.stdout = Tee(f"{OUTPUT_DIR}/log_{timestamp}.txt")

logger = logging.getLogger('hybrid')
logger.setLevel(logging.INFO)
logger.propagate = False

file_handler = logging.FileHandler(f'{LOGS_DIR}/hybrid.log')
file_handler.setLevel(logging.INFO)

# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–æ–≤
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∫ –ª–æ–≥–≥–µ—Ä—É
logger.addHandler(file_handler)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
reranker_model = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L-2-v2")

# —Ñ—É–Ω–∫—Ü–∏—è —É–¥–∞–ª—è–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
def clear_existing_collections(client):
    collections = client.get_collections().collections
    for collection in collections:
        client.delete_collection(collection.name)
        print(f"Collection {collection.name} has been cleared")

# —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
def create_hybrid_collection(client, collection_name):
    client.create_collection(
        collection_name=collection_name,
        vectors_config={
            "dense": VectorParams(
                size=768,
                distance=Distance.COSINE
            ),
            "colbertv2.0": VectorParams(
                size=128,
                distance=Distance.COSINE,
                multivector_config=MultiVectorConfig(comparator="max_sim")
            ),
        },
        sparse_vectors_config={
            "bm25": SparseVectorParams(
                index=SparseIndexParams(on_disk=False),
                modifier=Modifier.IDF
            )
        }
    )
    logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}, –≥–æ—Ç–æ–≤–∞ –∫ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—é")
    print(f"–°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}, –≥–æ—Ç–æ–≤–∞ –∫ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—é")

dense_embeddings = np.memmap('embeddings/dense_embeddings.memmap', dtype='float32', mode='r').reshape(-1, 768)
colbert_embeddings = np.memmap('embeddings/colbert_embeddings.memmap', dtype='float32', mode='r').reshape(-1, 256, 128)
with open('embeddings/sparse_embeddings.pkl', 'rb') as f:
    sparse_embeddings = pickle.load(f)

def build_point_from_files(
    item,
    idx,
    sparse_embeddings,
    dense_embeddings,
    colbert_embeddings
):
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ –∏–Ω–¥–µ–∫—Å—É
    sparse_embedding = sparse_embeddings[idx]
    dense_embedding = dense_embeddings[idx].tolist()
    colbert_embedding = colbert_embeddings[idx].tolist()

    return models.PointStruct(
        id=item["id"],
        payload=item,
        vector={
            "bm25": {
                "values": sparse_embedding.values.tolist() if sparse_embedding else [],
                "indices": sparse_embedding.indices.tolist() if sparse_embedding else []
            },
            "dense": dense_embedding,
            "colbertv2.0": colbert_embedding
        }
    )

# –∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏–Ω—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏
def upload_points_in_batches(client, collection_name, points, batch_size=50):
    for i in range(0, len(points), batch_size):
        batch = points[i:i + batch_size]
        client.upload_points(
            collection_name=collection_name,
            points=batch,
        )
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {i + len(batch)} –∏–∑ {len(points)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

# —Å–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
def upload_hybrid_data(client, collection_name: str, data):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Qdrant —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (BM25 + Dense + ColBERT)"""
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name} —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–∏—Å–∫–æ–º")
    clear_existing_collections(client)
    create_hybrid_collection(client, collection_name)
    # bm25_model, dense_model, colbert_model = load_embedding_models()
    logger.info(f"‚è≥ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ {collection_name}")
    print(f"‚è≥ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –∑–∞–≥—Ä—É–∑–∫–∏  {collection_name}")
    points = []
    for idx, item in tqdm(enumerate(data)):
        point = build_point_from_files(item, idx, sparse_embeddings, dense_embeddings, colbert_embeddings)
        points.append(point)
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(points)} points")
    upload_points_in_batches(client, collection_name, points)
    logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name}")
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name}")


#  –î–ª—è —Ç–µ—Å—Ç–∞ –º–æ–¥–µ–ª–∏
def load_embedding():
    return {
        "bm25": SparseTextEmbedding("Qdrant/bm25"),
        "dense": SentenceBERT("msmarco-distilbert-base-tas-b"),
        "colbert": LateInteractionTextEmbedding("colbert-ir/colbertv2.0")
    }

def encode_query(query_text, models):
    sparse_vector = list(models["bm25"].query_embed(query_text))
    sparse_embedding = sparse_vector[0] if sparse_vector else None
    dense_embedding = models["dense"].encode_corpus([{"text": query_text}], convert_to_tensor=False)[0]
    colbert_embedding = list(models["colbert"].embed(query_text))[0]

    return sparse_embedding, dense_embedding, colbert_embedding


def run_hybrid_search(client, collection_name, sparse_embedding, dense_embedding, colbert_embedding, top_k):
    prefetch = [
        models.Prefetch(query=dense_embedding, using="dense", limit=20),
        models.Prefetch(
            query=models.SparseVector(
                indices=sparse_embedding.indices.tolist() if sparse_embedding else [],
                values=sparse_embedding.values.tolist() if sparse_embedding else []
            ),
            using="bm25",
            limit=20
        ),
    ]

    start_time = time.time()
    search_results = client.query_points(
        collection_name,
        prefetch=prefetch,
        query=colbert_embedding,
        using="colbertv2.0",
        with_payload=True,
        limit=top_k
    )
    end_time = time.time()

    query_time = end_time - start_time
    found_contexts = [(point.payload.get('context', ''), point.score) for point in search_results.points]

    return found_contexts, query_time

def evaluate_accuracy(found_contexts, true_context, top_k_values, results, stage):
    if not found_contexts:
        return

    # –ü—Ä–æ–≤–µ—Ä–∏–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–æ—Ä—Ç–µ–∂–µ–º
    if not all(isinstance(item, (tuple, list)) and len(item) == 2 for item in found_contexts):
        raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç found_contexts: –æ–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (context, score), –ø–æ–ª—É—á–µ–Ω–æ: {found_contexts}")

    for k in top_k_values:
        results["accuracy"][stage][k]["total"] += 1
        top_k_contexts = [ctx for ctx, _ in found_contexts[:k]]
        if true_context in top_k_contexts:
            results["accuracy"][stage][k]["correct"] += 1


def update_speed_metrics(results):
    query_times = results["speed"]["query_times"]
    if query_times:
        results["speed"]["avg_time"] = sum(query_times) / len(query_times)
        results["speed"]["median_time"] = sorted(query_times)[len(query_times) // 2]
        results["speed"]["max_time"] = max(query_times)
        results["speed"]["min_time"] = min(query_times)
    del results["speed"]["query_times"]


def log_final_metrics(results, top_k_values):
    for stage in ["before_rerank", "after_rerank"]:
        for k in top_k_values:
            correct = results["accuracy"][stage][k]["correct"]
            total = results["accuracy"][stage][k]["total"]
            accuracy = correct / total if total > 0 else 0
            results["accuracy"][stage][k]["accuracy"] = accuracy
            logger.info(f"Hybrid Search {stage.replace('_', ' ')} (top-{k}): {accuracy:.4f} ({correct}/{total})")

    logger.info(f"Hybrid Search –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['speed']['avg_time'] * 1000:.2f} –º—Å")
    logger.info(f"Hybrid Search –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['speed']['median_time'] * 1000:.2f} –º—Å")
    logger.info(f"Hybrid Search –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['speed']['max_time'] * 1000:.2f} –º—Å")
    logger.info(f"Hybrid Search –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['speed']['min_time'] * 1000:.2f} –º—Å")


def benchmark_hybrid_rerank(client, collection_name, test_data, top_k_values=[1, 3], reranker=None):
    print(f"\nüîç –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ì–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ü–æ–∏—Å–∫–∞ + –†–µ—Ä–∞–Ω–∫–∞ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")
    logger.info(f"–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ì–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ü–æ–∏—Å–∫–∞ + –†–µ—Ä–∞–Ω–∫–∞ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")

    results = {
        "speed": {
            "avg_time": 0, "median_time": 0, "max_time": 0, "min_time": 0, "query_times": []
        },
        "accuracy": {
            "before_rerank": {k: {"correct": 0, "total": 0, "accuracy": 0} for k in top_k_values},
            "after_rerank": {k: {"correct": 0, "total": 0, "accuracy": 0} for k in top_k_values}
        }
    }

    max_top_k = max(top_k_values)
    total_queries = len(test_data)

    logger.info(f"–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Hybrid Search –¥–ª—è {total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤")
    print(f"‚è±Ô∏è  –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞...")

    embedding_models = load_embedding()
    progress_bar = tqdm(total=total_queries, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤", unit="–∑–∞–ø—Ä–æ—Å")

    for _, row in test_data.iterrows():
        query_text = row['question']
        true_context = row['context']

        sparse_embedding, dense_embedding, colbert_embedding = encode_query(query_text, embedding_models)
        found_contexts, query_time = run_hybrid_search(client, collection_name, sparse_embedding, dense_embedding, colbert_embedding, max_top_k)
        results["speed"]["query_times"].append(query_time)

        evaluate_accuracy(found_contexts, true_context, top_k_values, results, stage="before_rerank")

        if reranker:
            reranked_contexts = reranker(query_text, found_contexts)
            evaluate_accuracy(reranked_contexts, true_context, top_k_values, results, stage="after_rerank")

        progress_bar.update(1)

    progress_bar.close()

    update_speed_metrics(results)
    log_final_metrics(results, top_k_values)

    print(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Hybrid Search –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")
    return results

def reranker(query, candidates):
    texts = [(query, context) for context, _ in candidates]
    scores = reranker_model.predict(texts)

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–∏–º
    reranked_results = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ—Ä—Ç–µ–∂–∏: (context, new_score)
    return [(context, score) for (context, _), score in reranked_results]


def print_comparison(results_without_rerank, results_with_rerank, top_k_values=[1, 3]):
    print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Hybrid Search —Å —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º –∏ –±–µ–∑ –Ω–µ–≥–æ:\n")

    print("‚è≥ –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞:")
    print(f"  - –ë–µ–∑ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {results_without_rerank['speed']['avg_time'] * 1000:.2f} –º—Å")
    print(f"  - –° —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º: {results_with_rerank['speed']['avg_time'] * 1000:.2f} –º—Å")

    print("\nüéØ –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ (Accuracy):")
    for k in top_k_values:
        acc_before = results_without_rerank["accuracy"]["before_rerank"][k]["accuracy"]
        acc_after = results_with_rerank["accuracy"]["after_rerank"][k]["accuracy"]

        print(f"  - Top-{k}:")
        print(f"    - –ë–µ–∑ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {acc_before:.4f}")
        print(f"    - –° —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º: {acc_after:.4f}")


def visualize_results_rerank(results_without_rerank, results_with_rerank, top_k_values=[1, 3],
                             title_prefix="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º –∏ –±–µ–∑", save_dir=f"{GRAPHS_DIR}"):

    print(f"\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞...")
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")

    print(save_dir)

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    Path(save_dir).mkdir(exist_ok=True, parents=True)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–∏
    timestr = time.strftime("%Y%m%d_%H%M%S")  # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Å–∏–º–≤–æ–ª—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã

    # --- 1Ô∏è‚É£ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ---
    plt.figure(figsize=(10, 5))

    speeds = [
        results_without_rerank['speed']['avg_time'] * 1000,  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        results_with_rerank['speed']['avg_time'] * 1000
    ]

    bar_width = 0.8 /2
    n_groups = len(top_k_values)
    index = np.arange(n_groups)
    colors = plt.cm.tab10(np.linspace(0, 1, 2))
    labels = ["–ë–µ–∑ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞", "–° —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º"]  # –ü–æ–¥–ø–∏—Å–∏ —Å—Ç–æ–ª–±—Ü–æ–≤

    # # –°–æ–∑–¥–∞—ë–º –≥—Ä–∞—Ñ–∏–∫ –≤—Ä–µ–º–µ–Ω–∏

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã
    plt.bar(
        index,
        speeds,
        bar_width,
        color=colors,  # –¶–≤–µ—Ç–∞ —Å—Ç–æ–ª–±—Ü–æ–≤
        edgecolor='black',  # –¶–≤–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Å—Ç–æ–ª–±—Ü–æ–≤
        linewidth=0.5,  # –¢–æ–ª—â–∏–Ω–∞ –≥—Ä–∞–Ω–∏—Ü—ã —Å—Ç–æ–ª–±—Ü–æ–≤
    )

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞–¥ —Å—Ç–æ–ª–±—Ü–∞–º–∏
    for i, v in enumerate(speeds):
        if v > 0:
            plt.text(
                index[i],
                v + 1,
                f"{v:.1f}",
                ha='center',
                va='bottom',
                fontsize=6,
            )
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π
    plt.xticks(index, labels)
    plt.ylabel("–í—Ä–µ–º—è (–º—Å)")
    plt.title(f"{title_prefix}: –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞")

    # –°–µ—Ç–∫–∞ –¥–ª—è –æ—Å–∏ Y
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ (–¥–æ plt.show())
    speed_save_path = f"{save_dir}/speed_comparison_{timestr}_hybrid.png"
    plt.savefig(speed_save_path, dpi=300, bbox_inches='tight')

    # --- 2Ô∏è‚É£ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ ---
    plt.figure(figsize=(10, 5))
    acc_before = [results_without_rerank["accuracy"]["before_rerank"][k]["accuracy"] for k in top_k_values]
    acc_after = [results_with_rerank["accuracy"]["after_rerank"][k]["accuracy"] for k in top_k_values]

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    bar_width = 0.8 / 2
    n_groups = len(top_k_values)
    index = np.arange(n_groups)
    colors = plt.cm.tab10(np.linspace(0, 1, 2))
    labels = ["–ë–µ–∑ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞", "–° —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º"]

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.bar(
        index - bar_width / 2,
        acc_before,
        bar_width,
        label=labels[0],
        color=colors[0],
        edgecolor='black',
        linewidth=0.5,
    )

    plt.bar(
        index + bar_width / 2,
        acc_after,
        bar_width,
        label=labels[1],
        color=colors[1],
        edgecolor='black',
        linewidth=0.5,
    )

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞–¥ —Å—Ç–æ–ª–±—Ü–∞–º–∏
    for i, (v_before, v_after) in enumerate(zip(acc_before, acc_after)):
        if v_before > 0:
            plt.text(
                index[i] - bar_width / 2,
                v_before + 0.01,
                f"{v_before:.2f}",
                ha='center',
                va='bottom',
                fontsize=6,
            )
        if v_after > 0:
            plt.text(
                index[i] + bar_width / 2,
                v_after + 0.01,
                f"{v_after:.2f}",
                ha='center',
                va='bottom',
                fontsize=6,
            )

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π
    plt.xticks(index, [f"Top-{k}" for k in top_k_values])
    plt.ylabel("–¢–æ—á–Ω–æ—Å—Ç—å (Accuracy)")
    plt.title(f"{title_prefix}: –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞")

    # –õ–µ–≥–µ–Ω–¥–∞ –∏ —Å–µ—Ç–∫–∞
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    accuracy_save_path = f"{save_dir}/accuracy_comparison_{timestr}_hybrid.png"
    plt.savefig(accuracy_save_path, dpi=300, bbox_inches='tight')