import argparse
from qdrant_client.http.models import Distance, SearchParams, HnswConfigDiff
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from read_data_from_csv import read_data
from bench import benchmark_performance, visualize_results, benchmark_tfidf
from sklearn.feature_extraction.text import TfidfVectorizer
import logging
import time
from pathlib import Path
from tqdm import tqdm

from qdrant_client import models
import os

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ª–æ–≥–æ–≤ –∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
Path('./logs').mkdir(exist_ok=True)
Path('./logs/graphs').mkdir(exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –º–æ–¥—É–ª—è
logger = logging.getLogger('client')
logger.setLevel(logging.INFO)
logger.propagate = False  # –û—Ç–∫–ª—é—á–∞–µ–º –ø–µ—Ä–µ–¥–∞—á—É –ª–æ–≥–æ–≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–º –ª–æ–≥–≥–µ—Ä–∞–º

# –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ –ª–æ–≥–æ–≤ –≤ —Ñ–∞–π–ª
file_handler = logging.FileHandler('./logs/client.log')
file_handler.setLevel(logging.INFO)

# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–æ–≤
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∫ –ª–æ–≥–≥–µ—Ä—É
logger.addHandler(file_handler)

os.environ["TOKENIZERS_PARALLELISM"] = "false"


def parse_args():
    parser = argparse.ArgumentParser(description='–ë–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã')

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant
    parser.add_argument('--qdrant-host', type=str, default='localhost',
                        help='–•–æ—Å—Ç Qdrant —Å–µ—Ä–≤–µ—Ä–∞')
    parser.add_argument('--qdrant-port', type=int, default=6333,
                        help='–ü–æ—Ä—Ç Qdrant —Å–µ—Ä–≤–µ—Ä–∞')
    parser.add_argument('--collection-name', type=str, default='rag',
                        help='–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant')

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –ø–æ–∏—Å–∫–∞
    parser.add_argument('--model-names', nargs='+',
                        default=[
                            'all-MiniLM-L6-v2', 'paraphrase-multilingual-MiniLM-L12-v2', 'TF-IDF'],
                        help='–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è TF-IDF')
    parser.add_argument('--vector-size', type=int, default=384,
                        help='–†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤')
    parser.add_argument('--batch-size', type=int, default=100,
                        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--limit', type=int, default=100,
                        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è')

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã HNSW
    parser.add_argument('--hnsw-ef', type=int, default=16,
                        help='–ü–∞—Ä–∞–º–µ—Ç—Ä ef –¥–ª—è HNSW')
    parser.add_argument('--hnsw-m', type=int, default=16,
                        help='–ü–∞—Ä–∞–º–µ—Ç—Ä m –¥–ª—è HNSW (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π)')
    parser.add_argument('--ef-construct', type=int, default=200,
                        help='–ü–∞—Ä–∞–º–µ—Ç—Ä ef_construct –¥–ª—è HNSW')

    return parser.parse_args()


def create_collection(client, collection_name, vector_size, distance=Distance.COSINE):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant"""
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]

    if collection_name in collection_names:
        client.delete_collection(collection_name)
        logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É–¥–∞–ª–µ–Ω–∞")

    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏
    client.create_collection(
        collection_name=collection_name,
        vectors_config={
            "context": {
                "size": vector_size,
                "distance": distance
            }
        }
    )
    logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —Å–æ–∑–¥–∞–Ω–∞")


def upload_tfidf_data(client, collection_name, data, model):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö TF-IDF –≤ Qdrant —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤"""
    logger.info(
        f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name} —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]

    if collection_name in collection_names:
        client.delete_collection(collection_name)
        logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É–¥–∞–ª–µ–Ω–∞")

    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
    client.create_collection(
        collection_name=collection_name,
        vectors_config={},
        sparse_vectors_config={
            "text": models.SparseVectorParams(
                index=models.SparseIndexParams(
                    on_disk=False,
                )
            )
        },
    )
    logger.info(
        f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
    texts = [item["context"] for item in data]

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
    vectors = model.transform(texts)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ—á–∫–∏
    points = []
    for i in range(vectors.shape[0]):
        indices = vectors[i].indices.tolist()
        values = vectors[i].data.tolist()

        points.append(
            models.PointStruct(
                id=data[i]["id"],
                payload=data[i],
                vector={
                    'text': models.SparseVector(
                        indices=indices, values=values
                    )
                },
            )
        )

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
    client.upload_points(
        collection_name=collection_name,
        points=points,
        parallel=4,
        max_retries=3,
    )

    logger.info(
        f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö TF-IDF –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}")


def upload_data(client, collection_name, data, model, batch_size=100):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Qdrant"""
    logger.info(
        f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name}")
    start_time = time.time()

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
    progress_bar = tqdm(
        total=len(data), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", unit="–¥–æ–∫—É–º–µ–Ω—Ç")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞–º–∏
    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]
        texts = [item["context"] for item in batch]

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–∞—Ç—á–∞
        vectors = model.encode(texts, show_progress_bar=False)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–µ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        points = []
        for j, (item, vector) in enumerate(zip(batch, vectors)):
            points.append({
                "id": item["id"],
                "vector": {
                    "context": vector.tolist()
                },
                "payload": item
            })

        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant
        client.upsert(
            collection_name=collection_name,
            points=points
        )

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        progress_bar.update(len(batch))

        if (i + batch_size) % 1000 == 0 or (i + batch_size) >= len(data):
            logger.info(
                f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {min(i + batch_size, len(data))}/{len(data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
    progress_bar.close()

    elapsed_time = time.time() - start_time
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")


def main():
    args = parse_args()

    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–ø—É—Å–∫–µ –±–µ–Ω—á–º–∞—Ä–∫–∞
    print("\n" + "="*80)
    print("üöÄ –ó–ê–ü–£–°–ö –ë–ï–ù–ß–ú–ê–†–ö–ê RAG –°–ò–°–¢–ï–ú–´")
    print("="*80)
    logger.info("–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Qdrant
    client = QdrantClient(host=args.qdrant_host, port=args.qdrant_port)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å limit={args.limit}")
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (limit={args.limit})...")
    data_for_db, data_df = read_data(limit=args.limit)

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data_for_db)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data_for_db)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    all_models = args.model_names
    logger.info(f"–í—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {', '.join(all_models)}")
    print(f"üîÑ –í—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {', '.join(all_models)}")

    # –†–∞–∑–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–±—ã—á–Ω—ã–µ –∏ TF-IDF
    models_to_compare = [model for model in all_models if model != 'TF-IDF']
    use_tfidf = 'TF-IDF' in all_models

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
    model_instances = {}
    if models_to_compare:
        print(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤...")
        progress_bar = tqdm(total=len(models_to_compare),
                            desc="–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π", unit="–º–æ–¥–µ–ª—å")

        for model_name in models_to_compare.copy():
            try:
                logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: {model_name}")
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å SentenceTransformer
                model_instances[model_name] = SentenceTransformer(model_name)
                logger.info(f"–ú–æ–¥–µ–ª—å {model_name} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
                progress_bar.update(1)
            except Exception as e:
                logger.error(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                models_to_compare.remove(model_name)

        progress_bar.close()
        print(f"‚úÖ –ú–æ–¥–µ–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã: {', '.join(models_to_compare)}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TF-IDF –º–æ–¥–µ–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –æ–Ω–∞ –≤—ã–±—Ä–∞–Ω–∞
    tfidf_model = None
    if use_tfidf:
        print(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ TF-IDF...")
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ TF-IDF")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è TF-IDF
        corpus_texts = [item["context"] for item in data_for_db]

        # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å TF-IDF
        tfidf_model = TfidfVectorizer(stop_words='english', ngram_range=(
            1, 2), max_df=0.85, sublinear_tf=True)
        tfidf_model.fit(corpus_texts)
        logger.info("–ú–æ–¥–µ–ª—å TF-IDF –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        print(f"‚úÖ –ú–æ–¥–µ–ª—å TF-IDF –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤
    search_algorithms = {
        "Exact Search": SearchParams(exact=True),
        f"HNSW Users ef={args.hnsw_ef}": SearchParams(hnsw_ef=args.hnsw_ef),
        "HNSW High Precision ef=512": SearchParams(hnsw_ef=512)
    }

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤
    speed_results = {}
    accuracy_results = {}

    # –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å dense –≤–µ–∫—Ç–æ—Ä–∞–º–∏
    if models_to_compare:
        for model_name in models_to_compare:
            model = model_instances[model_name]

            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_name = f"{args.collection_name}_{model_name.replace('-', '_')}"
            create_collection(client, collection_name, args.vector_size)

            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            upload_data(client, collection_name,
                        data_for_db, model, args.batch_size)

            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
            speed_results[model_name] = {}
            accuracy_results[model_name] = {}

            # –û—Ü–µ–Ω–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
            for algo_name, search_params in search_algorithms.items():
                logger.info(
                    f"–û—Ü–µ–Ω–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ {algo_name} —Å –º–æ–¥–µ–ª—å—é {model_name}")
                print(
                    f"\nüîç –û—Ü–µ–Ω–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ {algo_name} —Å –º–æ–¥–µ–ª—å—é {model_name}")

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ HNSW
                if algo_name.startswith("HNSW"):
                    client.update_collection(
                        collection_name=collection_name,
                        hnsw_config=HnswConfigDiff(
                            m=args.hnsw_m,
                            ef_construct=args.ef_construct,
                        )
                    )

                # –ó–∞–ø—É—Å–∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                benchmark_results = benchmark_performance(
                    client=client,
                    collection_name=collection_name,
                    test_data=data_df,
                    model=model,
                    search_params=search_params,
                    top_k_values=[1, 3]
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–æ—Ä–æ—Å—Ç–∏
                speed_results[model_name][algo_name] = benchmark_results["speed"]

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ—á–Ω–æ—Å—Ç–∏
                accuracy_results[model_name][algo_name] = benchmark_results["accuracy"]

    # –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è TF-IDF, –µ—Å–ª–∏ –æ–Ω–∞ –≤—ã–±—Ä–∞–Ω–∞
    tfidf_results = None
    if use_tfidf and tfidf_model:
        print("\n" + "="*80)
        print("üîç –û–¶–ï–ù–ö–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò TF-IDF")
        print("="*80)
        logger.info("–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ TF-IDF")

        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è TF-IDF
        tfidf_collection_name = f"{args.collection_name}_tfidf"

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö TF-IDF
        upload_tfidf_data(client, tfidf_collection_name,
                          data_for_db, tfidf_model)

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è TF-IDF
        tfidf_speed_results = {}
        tfidf_accuracy_results = {}

        # –û—Ü–µ–Ω–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –ø–æ–∏—Å–∫–∞
        for algo_name, search_params in search_algorithms.items():
            logger.info(f"–û—Ü–µ–Ω–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ {algo_name} —Å –º–æ–¥–µ–ª—å—é TF-IDF")
            print(f"\nüîç –û—Ü–µ–Ω–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ {algo_name} —Å –º–æ–¥–µ–ª—å—é TF-IDF")

            # –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è TF-IDF —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ–∏—Å–∫–∞
            benchmark_results = benchmark_tfidf(
                client=client,
                collection_name=tfidf_collection_name,
                test_data=data_df,
                model=tfidf_model,
                search_params=search_params,
                top_k_values=[1, 3]
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–æ—Ä–æ—Å—Ç–∏
            tfidf_speed_results[algo_name] = benchmark_results["speed"]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ—á–Ω–æ—Å—Ç–∏
            tfidf_accuracy_results[algo_name] = benchmark_results["accuracy"]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã TF-IDF –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        tfidf_results = {
            "speed": tfidf_speed_results,
            "accuracy": tfidf_accuracy_results
        }

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∫–æ—Ä–æ—Å—Ç–∏
    print("\n" + "="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –°–ö–û–†–û–°–¢–ò –ü–û–ò–°–ö–ê")
    print("="*80)

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤
    if models_to_compare:
        for model_name in models_to_compare:
            print(f"\n–ú–æ–¥–µ–ª—å: {model_name}")

            for algo_name in speed_results[model_name].keys():
                result = speed_results[model_name][algo_name]

                print(f"  –ê–ª–≥–æ—Ä–∏—Ç–º: {algo_name}")
                print(f"    –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {result['avg_time'] * 1000:.2f} –º—Å")
                print(
                    f"    –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è: {result['median_time'] * 1000:.2f} –º—Å")
                print(
                    f"    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {result['max_time'] * 1000:.2f} –º—Å")
                print(
                    f"    –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {result['min_time'] * 1000:.2f} –º—Å")

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è TF-IDF
    if use_tfidf and tfidf_results:
        print(f"\n–ú–æ–¥–µ–ª—å: TF-IDF")

        for algo_name in tfidf_results["accuracy"].keys():
            print(f"  –ê–ª–≥–æ—Ä–∏—Ç–º: {algo_name}")

            for k in [1, 3]:
                if k in tfidf_results["accuracy"][algo_name]:
                    result = tfidf_results["accuracy"][algo_name][k]
                    print(
                        f"    Top-{k}: –¢–æ—á–Ω–æ—Å—Ç—å = {result['accuracy']:.4f} ({result['correct']}/{result['total']})")

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–æ—á–Ω–æ—Å—Ç–∏
    print("\n" + "="*80)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –¢–û–ß–ù–û–°–¢–ò –ü–û–ò–°–ö–ê")
    print("="*80)

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤
    if models_to_compare:
        for model_name in models_to_compare:
            print(f"\n–ú–æ–¥–µ–ª—å: {model_name}")

            for algo_name in accuracy_results[model_name].keys():
                print(f"  –ê–ª–≥–æ—Ä–∏—Ç–º: {algo_name}")

                for k in [1, 3]:
                    if k in accuracy_results[model_name][algo_name]:
                        result = accuracy_results[model_name][algo_name][k]
                        print(
                            f"    Top-{k}: –¢–æ—á–Ω–æ—Å—Ç—å = {result['accuracy']:.4f} ({result['correct']}/{result['total']})")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if (models_to_compare or use_tfidf):
        visualize_results(
            speed_results=speed_results,
            accuracy_results=accuracy_results,
            tfidf_results=tfidf_results,
            title_prefix="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ RAG —Å–∏—Å—Ç–µ–º—ã",
            save_dir="./logs/graphs"
        )

    logger.info("–ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    print("\n" + "="*80)
    print("‚úÖ –ë–ï–ù–ß–ú–ê–†–ö –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û")
    print("–ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ ./logs/graphs/")
    print("="*80)


if __name__ == "__main__":
    main()
