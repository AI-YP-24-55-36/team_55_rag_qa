import time
from tqdm import tqdm
import numpy as np
from qdrant_client.models import (
    Distance,
    VectorParams,
    PointStruct
)
from sentence_transformers import SentenceTransformer
from logger_init import setup_paths, setup_logging
from report_data import (init_results, evaluate_accuracy,
                         calculate_speed_stats, compute_final_accuracy,
                         log_topk_accuracy, log_speed_stats)

BASE_DIR, LOGS_DIR, GRAPHS_DIR, OUTPUT_DIR, EMBEDDINGS_DIR = setup_paths()
logger = setup_logging(LOGS_DIR, OUTPUT_DIR)

# —Å–ø–∏—Å–æ–∫ –¥–µ–Ω–∑ –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–∞–º–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤
MODEL_VECTOR_SIZES = {
    'msmarco-roberta-base-ance-firstp': 768,
    'all-MiniLM-L6-v2': 384,
    'msmarco-MiniLM-L-6-v3': 384,
}


# —Å–æ–∑–¥–∞–Ω–∏–µ –¥–µ–Ω–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
def create_collection(client, collection_name, vector_size, distance=Distance.COSINE):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant"""
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]

    if collection_name in collection_names:
        client.delete_collection(collection_name)
        logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É–¥–∞–ª–µ–Ω–∞")

    client.create_collection(
        collection_name=collection_name,
        vectors_config={
            "context": VectorParams(size=vector_size, distance=distance)
        }
    )
    logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —Å–æ–∑–¥–∞–Ω–∞")


# —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∏–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î
def build_point_from_memmap(item, idx, vectors):
    vector = vectors[idx].tolist()
    return PointStruct(
        id=item["id"],
        payload=item,
        vector={
            "context": vector
        }
    )


# –∑–∞–∫–≥—Ä—É–∑–∫–∞ –ø–æ–∏–Ω—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏
def upload_points_in_batches(client, collection_name, points, batch_size=50):
    for i in range(0, len(points), batch_size):
        batch = points[i:i + batch_size]
        client.upload_points(
            collection_name=collection_name,
            points=batch,
        )
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {i + len(batch)} –∏–∑ {len(points)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")


# —á—Ç–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤ –±–¥
def upload_dense_data(client, collection_name, data, dim, embedding_name: str, batch_size=1,
                      embedding_dir="embeddings", dtype='float32'):
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name} —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ {embedding_name}")
    start_time = time.time()

    memmap_path = f"{embedding_dir}/{embedding_name}.memmap"
    # —á—Ç–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
    vectors = np.memmap(memmap_path, dtype=dtype, mode='r').reshape(-1, dim)
    points = []
    progress_bar = tqdm(total=len(data), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–µ–∫", unit="–¥–æ–∫—É–º–µ–Ω—Ç")
    # –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏–Ω—Ç–æ–≤
    for idx, item in enumerate(data):
        point = build_point_from_memmap(item, idx, vectors)
        points.append(point)
        progress_bar.update(1)
    progress_bar.close()
    logger.info(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ {len(points)} —Ç–æ—á–µ–∫ –≤ Qdrant...")
    upload_points_in_batches(client, collection_name, points, batch_size=batch_size)
    elapsed_time = time.time() - start_time
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")


#  —Å–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –ë–î
def upload_dense_model_collections(client, models_to_compare, args, data_for_db):
    for model_name in models_to_compare:
        collection_name = f"{args.collection_name}_{model_name.replace('-', '_')}"
        vector_size = MODEL_VECTOR_SIZES.get(model_name)
        if vector_size is None:
            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ... –ü—Ä–æ–ø—É—Å–∫.")
            continue
        logger.info(f"\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {collection_name}")
        create_collection(client, collection_name, vector_size)
        logger.info(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏: {model_name}")
        upload_dense_data(
            client=client,
            collection_name=collection_name,
            data=data_for_db,
            dim=vector_size,
            embedding_name=model_name,
            batch_size=args.batch_size,
            dtype='float32'
        )


# –ø–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
def run_query(client, collection_name, query_vector, search_params, limit):
    start_time = time.time()
    search_results = client.query_points(
        collection_name=collection_name,
        query=query_vector.tolist(),
        using="context",
        search_params=search_params,
        limit=limit
    )
    end_time = time.time()
    return search_results, end_time - start_time

# –∑–∞–º–µ—Ä—ã —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
def benchmark_performance(client, collection_name, test_data, model_name, search_params=None, top_k_values=[1, 3]):
    print(f"\nüîç –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")
    logger.info(f"–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")
    results = init_results(top_k_values)
    max_top_k = max(top_k_values)
    total_queries = len(test_data)
    model = SentenceTransformer(model_name)
    logger.info(f"–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è {total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤")
    print(f"‚è±Ô∏è  –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞...")
    progress_bar = tqdm(total=total_queries, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤", unit="–∑–∞–ø—Ä–æ—Å")

    for idx, row in test_data.iterrows():
        query_text = row['question']
        true_context = row['context']
        query_vector = model.encode(query_text)
        search_results, query_time = run_query(client, collection_name, query_vector, search_params, max_top_k)
        results["speed"]["query_times"].append(query_time)
        found_contexts = [point.payload.get('context', '') for point in search_results.points]
        evaluate_accuracy(results["accuracy"], found_contexts, true_context, top_k_values, query_text, idx)
        progress_bar.update(1)

    progress_bar.close()
    calculate_speed_stats(results)
    compute_final_accuracy(results)
    log_topk_accuracy(results, top_k_values)
    log_speed_stats(results)
    print(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")
    return results
