import numpy as np
from tqdm import tqdm
from fastembed import SparseTextEmbedding, LateInteractionTextEmbedding
from beir.retrieval.models import SentenceBERT
import pickle
from read_data_from_csv import read_data
from logger_init import setup_paths, setup_logging

BASE_DIR, LOGS_DIR, GRAPHS_DIR, OUTPUT_DIR, EMBEDDINGS_DIR = setup_paths()
logger = setup_logging(LOGS_DIR, OUTPUT_DIR)

'''
–í –∫–∞—á–µ—Å—Ç–≤–µ dense –º–æ–¥–µ–ª–∏ –±–µ—Ä–µ–º msmarco-distilbert-base-tas-b
–ú–µ—Ç–æ–¥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –≤ —Å—Ç–∞—Ç—å–µ Sbert.net - TAS-B: Improving Dense Retrieval with Token-Averaged Embeddings (https://www.sbert.net/examples/research/tas-b/README.html), 
–æ—Ç–ª–∏—á–∏—è –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞:
–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ [CLS] —Ç–æ–∫–µ–Ω –∫–∞–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—á—Ç–æ —á–∞—Å—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –≤ BERT), TAS-B —É—Å—Ä–µ–¥–Ω—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤.
–≠—Ç–æ –¥–∞—ë—Ç –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è retrieval-–∑–∞–¥–∞—á.
–ü–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —Ç–æ—á–Ω–æ—Å—Ç—å dense retrieval –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏.

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –±–æ–ª—å—à–∏–º –∫–æ–ª–ª–µ–∫—Ü–∏—è–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–í–æ–ø—Ä–æ—Å - –æ—Ç–≤–µ—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã(QA)
–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
'''


def load_embedding_models():
    bm25_model = SparseTextEmbedding("Qdrant/bm25")
    colbert_model = LateInteractionTextEmbedding("colbert-ir/colbertv2.0")

    # —Å–ø–∏—Å–æ–∫ dense –º–æ–¥–µ–ª–µ–π
    dense_models = {
        "tas_b": SentenceBERT("msmarco-distilbert-base-tas-b"),  # 768
        "all-MiniLM-L6-v2": SentenceBERT("sentence-transformers/all-MiniLM-L6-v2"),  # 384
        "msmarco-MiniLM-L-6-v3": SentenceBERT("sentence-transformers/msmarco-MiniLM-L-6-v3"),  # 384
        "msmarco-roberta-base-ance-firstp": SentenceBERT("sentence-transformers/msmarco-roberta-base-ance-firstp"),  # 768
    }

    return bm25_model, colbert_model, dense_models


def build_embeddings(item, bm25_model, colbert_model, dense_models):
    text = item["context"]

    # BM25 sparse vector
    sparse_vector = list(bm25_model.query_embed(text))
    sparse_embedding = sparse_vector[0] if sparse_vector else None

    # Dense vectors (–≤—Å–µ –º–æ–¥–µ–ª–∏)
    dense_embeddings = {}
    for model_name, model in dense_models.items():
        emb = model.encode_corpus([{"text": text}], convert_to_tensor=False)[0]
        dense_embeddings[model_name] = emb

    # ColBERT vector
    colbert_embedding = list(colbert_model.embed(text))[0]

    return sparse_embedding, dense_embeddings, colbert_embedding


def generate_emb():
    sparse_list = []
    colbert_list = []
    dense_dict = {name: [] for name in dense_models}

    data_for_db, data_df = read_data(limit=11000)

    for item in tqdm(data_for_db):
        sparse_embedding, dense_embeddings, colbert_embedding = build_embeddings(
            item, bm25_model, colbert_model, dense_models
        )

        sparse_list.append(sparse_embedding)
        colbert_list.append(colbert_embedding)

        for name, emb in dense_embeddings.items():
            dense_dict[name].append(emb)

    return sparse_list, dense_dict, colbert_list


def pad_to_fixed_length(matrix, target_len=512):
    current_len = matrix.shape[0]
    dim = matrix.shape[1]

    if current_len > target_len:
        # –æ–±—Ä–µ–∑–∞–µ–º, –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
        return matrix[:target_len]
    elif current_len < target_len:
        # –¥–æ–±–∞–≤–∏–º –Ω—É–ª–∏
        pad = np.zeros((target_len - current_len, dim), dtype=matrix.dtype)
        return np.vstack((matrix, pad))
    else:
        return matrix


def memmap_emb(
        output_dir=EMBEDDINGS_DIR,
        colbert_tokens=256,
        dtype='float32'
):
    sparse, dense_dict, colbert = generate_emb()

    num_texts = len(colbert)
    colbert_shape = colbert[0].shape
    tokens, colbert_dim = colbert_shape

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º dense —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    for model_name, dense_vectors in dense_dict.items():
        dense_dim = len(dense_vectors[0])
        dense_output_path = f"{output_dir}/{model_name}.memmap"
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º dense —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ({model_name}) –≤ {dense_output_path}...")

        dense_memmap = np.memmap(dense_output_path, dtype=dtype, mode='w+', shape=(num_texts, dense_dim))
        for idx, vector in enumerate(dense_vectors):
            dense_memmap[idx] = vector
        del dense_memmap

    # ColBERT
    colbert_output_path = f"{output_dir}/colbert_embeddings.memmap"
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º ColBERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ {colbert_output_path}...")
    colbert_memmap = np.memmap(colbert_output_path, dtype=dtype, mode='w+',
                               shape=(num_texts, colbert_tokens, colbert_dim))
    for idx, matrix in enumerate(colbert):
        padded_matrix = pad_to_fixed_length(matrix, target_len=colbert_tokens)
        colbert_memmap[idx] = padded_matrix
    del colbert_memmap

    # Sparse
    sparse_output_path = f"{output_dir}/sparse_embeddings.pkl"
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º sparse —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (BM25) –≤ {sparse_output_path}...")
    with open(sparse_output_path, 'wb') as f:
        pickle.dump(sparse, f)

    print("‚úÖ –í—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")


if __name__ == '__main__':
    bm25_model, colbert_model, dense_models = load_embedding_models()
    memmap_emb()
