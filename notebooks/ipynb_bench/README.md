# Бенчмарк RAG системы

Этот инструмент позволяет сравнивать производительность различных моделей эмбеддингов и алгоритмов поиска для Retrieval-Augmented Generation (RAG) систем.

Демо-скринкаст работы скрипта `bench.py`  


## Возможности

- Сравнение различных dense моделей:
``` {
        'msmarco-roberta-base-ance-firstp': 768,
        'all-MiniLM-L6-v2': 384,
        'msmarco-MiniLM-L-6-v3': 384,
    }

```
- Сравнение BM25 (sparse model)
- Оценка скорости и точности поиска
- Визуализация результатов в виде графиков
- Поддержка различных алгоритмов поиска (Exact Search, HNSW (Hierarchical Navigable Small World) для dense моделей)
- Поддержка гибрибного поиска в коллекции с разными видами векторов
- Сравнение результатов с реранкингом ( `sentence_transformers CrossEncoder`) и без реранкинга 

## Данные

- Тестирование производилось на датасете с 11000 записями
- Dense эмбеддинги вычисляются перед запуском бенчмарка и сохранены в numpy memmap
- Sparse эмбеддинги сохранены в pkl

## Требования

- Python 3.10+
- Qdrant (запущенный локально или удаленно)
- Необходимые библиотеки (см. `requirements.txt`)

## Установка

```bash
# Установка зависимостей
pip install -r requirements.txt
```

## Использование

### Запуск подсчета эмбеддингов

```
python embeddings_compute.py
```
вычисление производится сразу для всех тестируемых моделей для полного объема данных (процесс занимает примерно 1 час 30 минут)


### Базовый запуск

```bash
python bench.py
```

### Параметры командной строки

```bash
--qdrant-host  # Хост Qdrant сервера
--qdrant-port # Порт Qdrant сервера
--collection-name # Название коллекции в Qdrant
--topk #Количество извлекаемых ответов
--model-names  # Список моделей для сравнения
--batch-size # 'Размер батча для загрузки данных'
--limit #  количество записей для проведения тестирования
```

### Основные параметры

| Параметр | Описание | Значение по умолчанию                                         |
|----------|----------|---------------------------------------------------------------|
| `--model-names` | Список моделей для сравнения | `msmarco-roberta-base-ance-firstp all-MiniLM-L6-v2 msmarco-MiniLM-L-6-v3 BM25` |
| `--limit` | Максимальное количество записей для использования | `11000`                                                         |
| `--qdrant-host` | Хост Qdrant сервера | `localhost`                                                   |
| `--qdrant-port` | Порт Qdrant сервера | `6333`                                                        |
| `--collection-name` | Название коллекции в Qdrant | `rag`                                                         |

### Параметры HNSW

| Параметр | Описание | Значение по умолчанию |
|----------|----------|------------------------|
| `--hnsw-ef` | Параметр ef для HNSW | `16` |
| `--hnsw-m` | Параметр m для HNSW (количество соседей) | `16` |
| `--ef-construct` | Параметр ef_construct для HNSW | `200` |

```
python bench.py --model-names BM25 msmarco-roberta-base-ance-firstp --topk 1 3 --limit 11000
python bench.py --hybrid 1 --limit 11000
                        
```
В результате получаем два графика для одного бенча:

- сравнение по скорости поиска по трем методам поиска:

- сравнение по точности поиска между разными моделями конвертации текстов в эмбеддинги и методами поиска

- при  запуски модели с параметром `--hybrid 1` активируется гибридная модель с реранкингом, в результате получаем сравнение:

результаты сохраняются в файл:

Коллекции в БД:


### Reranker

Используется простейшее ранжирование - сортировка кандидатов

`reranked_results = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)`  
Кандидаты и их соответствующие оценки объединяются в список кортежей с помощью zip. 
Затем этот список сортируется по оценкам (второй элемент каждого кортежа), в порядке убывания (reverse=True). 
Это означает, что кандидаты с более высокими оценками (т.е., более релевантные) будут расположены в начале.

## Примеры использования

Для запуска предустановленного пайплайна обучения набора моделей используется баш-скрипт `example.sh`,
он позволяет запустить и сравнить несколько вариантов работы модели с разной векторизацией и разными параметрами `HNSW`, гибридный поиск с реранкингом и без 


### Сравнение dense моделей

```bash
python bench.py --model-names msmarco-roberta-base-ance-firstp all-MiniLM-L6-v2 msmarco-MiniLM-L-6-v3
```
### Сравнение всех моделей

```bash
python bench.py --model-names msmarco-roberta-base-ance-firstp all-MiniLM-L6-v2 msmarco-MiniLM-L-6-v3
```
### Уменьшение размера выборки

```bash
python bench.py --limit 1000
```

### Настройка параметров HNSW

```bash
python bench.py --hnsw-ef 128 --hnsw-m 32 --ef-construct 400
```

## Результаты

Результаты бенчмарка сохраняются:

- Логи:  `./logs/bench.log` и `./logs/log_timestamp.txt`
- Графики: `./logs/graphs/`

## Структура проекта

- `bench.py` - основной скрипт для запуска бенчмарка
- `read_data_from_csv.py` - функции для чтения данных
- `config.yml` - содержит пути к папкам для сохранения логов и картинок
- `load_config.py` - загружает конфигурацию путей
- `logger_init.py` - инициализация логгирования
- `embeddingы_compute.py` - подсчет эмбеддингов
- `hybrid_rerank.py` - создание гибридной коллекции, запуск гибридного поиска и реранкинг
- `dense_model.py` - функции для работы с dense моделями (чтение данных/создание точек загрузки/загрузка в БД/рассчет точности и скорости)
- `sparse_bm25.py` - функции для работы со Sparse моделью (чтение данных/создание точек загрузки/загрузка в БД/рассчет точности и скорости)
- `report_data.py` - печать результатов в лог и на экран
- `visualisation.py` - отрисовка графиков сравнения моделей и видов поиска

## Примечания
- Для корректной работы необходим запущенный сервер Qdrant
- Для больших наборов данных рекомендуется увеличить значение `--batch-size` 

## Выводы:

1. Гибридный поиск с реранкингом работает медленне остальных, но точность выше
2. Exact_Search работает медленнее HNSW
2. Sparce вектора извлекаются быстрее, чем dense
3. Лучшая точность у Sparce векторов (bm25)